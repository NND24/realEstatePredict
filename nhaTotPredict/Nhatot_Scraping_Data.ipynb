{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Bypass detection\n",
    "chrome_options.add_argument(\"--window-size=1,1\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\")  # User-agent string\n",
    "\n",
    "# Disable automation controls so that websites don't detect Selenium\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links mua bán nhà ở from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 1000\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/mua-ban-nha-dat?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('muaBanNhaOLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links mua bán căn hộ chung cư from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 100\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/mua-ban-can-ho-chung-cu-tp-ho-chi-minh?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('muaBanCanHoChungCuLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links mua bán mặt bằng from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 200\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/sang-nhuong-van-phong-mat-bang-kinh-doanh-tp-ho-chi-minh?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('muaBanMatBangLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links mua bán đất from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 200\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/mua-ban-dat-tp-ho-chi-minh?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('muaBanDatLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links cho thuê căn hộ chung cư from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 1000\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/thue-can-ho-chung-cu?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('choThueCanHoChungCuLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links cho thuê nhà ở from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 1000\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/thue-nha-dat?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('choThueNhaOLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links cho thuê mặt bằng from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 1000\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/thue-van-phong-mat-bang-kinh-doanh?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('choThueMatBangLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links cho thuê đất from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 1000\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/thue-dat?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('choThueDatLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl links cho thuê phòng trọ from nhatot.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array to store the links\n",
    "arr = []\n",
    "start_page = 1\n",
    "end_page = 1000\n",
    "\n",
    "# Retry logic\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "for page in range(start_page, end_page + 1):\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # Initialize WebDriver for each page\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            url = f\"https://www.nhatot.com/thue-phong-tro?page={page}\"\n",
    "            driver.get(url)\n",
    "\n",
    "            # Use explicit wait for better control\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, 'AdItem_adItem__gDDQT')))\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find all the links with the specified class\n",
    "            a_tags = soup.find_all(\"a\", class_='AdItem_adItem__gDDQT')\n",
    "\n",
    "            # Append the full link to the array\n",
    "            for a in a_tags:\n",
    "                full_link = \"https://www.nhatot.com\" + a['href']\n",
    "                arr.append(full_link)\n",
    "\n",
    "            driver.quit()\n",
    "            break  # Break if the request was successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            driver.quit()\n",
    "            if attempt + 1 == MAX_RETRIES:\n",
    "                print(\"Max retries reached. Skipping this page.\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(arr, columns=[\"Links\"])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('choThuePhongTroLinks.csv', index=True)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lấy dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "# Truy cập vào trang web\n",
    "url = 'https://www.nhatot.com/mua-ban-nha-dat-thanh-pho-bien-hoa-dong-nai/119225036.htm#px=SR-stickyad-[PO-1][PL-top]'\n",
    "driver.get(url)\n",
    "\n",
    "# Đợi một chút để trang tải hết nội dung\n",
    "time.sleep(5)\n",
    "\n",
    "# Lấy nội dung của trang\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Đóng trình duyệt\n",
    "driver.quit()\n",
    "\n",
    "# Phân tích cú pháp HTML với BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "print(price)\n",
    "\n",
    "address = soup.find_all(\"span\", class_=\"bwq0cbs\")\n",
    "print(address[4].text)\n",
    "\n",
    "price_m2 = soup.find(\"span\", itemprop=\"price_m2\").text\n",
    "rooms = soup.find(\"span\", itemprop=\"rooms\").text\n",
    "toilets = soup.find(\"span\", itemprop=\"toilets\").text\n",
    "direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "floors = soup.find(\"span\", itemprop=\"floors\").text\n",
    "property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "house_type = soup.find(\"span\", itemprop=\"house_type\").text\n",
    "furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "width = soup.find(\"span\", itemprop=\"width\").text\n",
    "length = soup.find(\"span\", itemprop=\"length\").text\n",
    "living_size = soup.find(\"span\", itemprop=\"living_size\").text\n",
    "size = soup.find(\"span\", itemprop=\"size\").text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lấy chi tiết data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data mua bán nhà ở"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, wait_fixed, stop_after_attempt\n",
    "\n",
    "# Function to extract data from a single page\n",
    "@retry(wait=wait_fixed(2), stop=stop_after_attempt(3))  # Wait 2 seconds between retries, retry 3 times\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[4].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            price_m2 = soup.find(\"span\", itemprop=\"price_m2\").text\n",
    "        except AttributeError:\n",
    "            price_m2 = None\n",
    "\n",
    "        try:\n",
    "            rooms = soup.find(\"span\", itemprop=\"rooms\").text\n",
    "        except AttributeError:\n",
    "            rooms = None\n",
    "\n",
    "        try:\n",
    "            toilets = soup.find(\"span\", itemprop=\"toilets\").text\n",
    "        except AttributeError:\n",
    "            toilets = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "\n",
    "        try:\n",
    "            floors = soup.find(\"span\", itemprop=\"floors\").text\n",
    "        except AttributeError:\n",
    "            floors = None\n",
    "\n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            house_type = soup.find(\"span\", itemprop=\"house_type\").text\n",
    "        except AttributeError:\n",
    "            house_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "        except AttributeError:\n",
    "            furnishing_sell = None\n",
    "\n",
    "        try:\n",
    "            width = soup.find(\"span\", itemprop=\"width\").text\n",
    "        except AttributeError:\n",
    "            width = None\n",
    "\n",
    "        try:\n",
    "            length = soup.find(\"span\", itemprop=\"length\").text\n",
    "        except AttributeError:\n",
    "            length = None\n",
    "\n",
    "        try:\n",
    "            living_size = soup.find(\"span\", itemprop=\"living_size\").text\n",
    "        except AttributeError:\n",
    "            living_size = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "\n",
    "        try:\n",
    "            pty_characteristics = soup.find(\"span\", itemprop=\"pty_characteristics\").text\n",
    "        except AttributeError:\n",
    "            pty_characteristics = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"price_m2\": price_m2,\n",
    "            \"rooms\": rooms,\n",
    "            \"toilets\": toilets,\n",
    "            \"direction\": direction,\n",
    "            \"floors\": floors,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"house_type\": house_type,\n",
    "            \"furnishing_sell\": furnishing_sell,\n",
    "            \"width\": width,\n",
    "            \"length\": length,\n",
    "            \"living_size\": living_size,\n",
    "            \"size\": size,\n",
    "            \"pty_characteristics\": pty_characteristics,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='muaBanNhaDat3.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('nhaDatLinks1.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    i = 0\n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        i+=1\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL {i}: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = pd.read_csv('muaBanNhaDat.csv')\n",
    "frame1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data mua bán căn hộ chung cư"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed URL 1: https://www.nhatot.com/mua-ban-can-ho-chung-cu-thanh-pho-thu-duc-tp-ho-chi-minh/119555664.htm#px=SR-stickyad-[PO-1][PL-top]\n",
      "Processed URL 2: https://www.nhatot.com/mua-ban-can-ho-chung-cu-quan-7-tp-ho-chi-minh/119593013.htm#px=SR-stickyad-[PO-2][PL-top]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x0000024650CA8540>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Workspace python\\RealEstatePredict\\venv\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 189, in __del__\n",
      "    self.stop()\n",
      "  File \"d:\\Workspace python\\RealEstatePredict\\venv\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"d:\\Workspace python\\RealEstatePredict\\venv\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 126, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"C:\\python312\\Lib\\urllib\\request.py\", line 215, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\python312\\Lib\\urllib\\request.py\", line 515, in open\n",
      "    response = self._open(req, data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\python312\\Lib\\urllib\\request.py\", line 532, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\python312\\Lib\\urllib\\request.py\", line 492, in _call_chain\n",
      "    result = func(*args)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"C:\\python312\\Lib\\urllib\\request.py\", line 1373, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\python312\\Lib\\urllib\\request.py\", line 1344, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"C:\\python312\\Lib\\http\\client.py\", line 1331, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\python312\\Lib\\http\\client.py\", line 1377, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\python312\\Lib\\http\\client.py\", line 1326, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\python312\\Lib\\http\\client.py\", line 1085, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\python312\\Lib\\http\\client.py\", line 1029, in send\n",
      "    self.connect()\n",
      "  File \"C:\\python312\\Lib\\http\\client.py\", line 995, in connect\n",
      "    self.sock = self._create_connection(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\python312\\Lib\\socket.py\", line 844, in create_connection\n",
      "    exceptions.clear()  # raise only the last error\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed URL 3: https://www.nhatot.com/mua-ban-can-ho-chung-cu-thanh-pho-thu-duc-tp-ho-chi-minh/119269431.htm#px=SR-stickyad-[PO-3][PL-top]\n"
     ]
    }
   ],
   "source": [
    "def extract_data(url):\n",
    "    # Setup Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Bypass detection\n",
    "    chrome_options.add_argument(\"--window-size=1,1\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\")  # User-agent string\n",
    "\n",
    "    # Disable automation controls so that websites don't detect Selenium\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    # Initialize WebDriver with Chrome options\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load and elements to be available\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'cd9gm5n')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            title = soup.find(\"title\").text\n",
    "        except AttributeError:\n",
    "            title = None\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find(\"span\", class_=\"flex-1\").text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            price_m2 = soup.find(\"span\", itemprop=\"price_m2\").text\n",
    "        except AttributeError:\n",
    "            price_m2 = None\n",
    "\n",
    "        try:\n",
    "            rooms = soup.find(\"span\", itemprop=\"rooms\").text\n",
    "        except AttributeError:\n",
    "            rooms = None\n",
    "\n",
    "        try:\n",
    "            toilets = soup.find(\"span\", itemprop=\"toilets\").text\n",
    "        except AttributeError:\n",
    "            toilets = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "\n",
    "        try:\n",
    "            property_status = soup.find(\"span\", itemprop=\"property_status\").text\n",
    "        except AttributeError:\n",
    "            property_status = None\n",
    "            \n",
    "        try:\n",
    "            balconydirection = soup.find(\"span\", itemprop=\"balconydirection\").text\n",
    "        except AttributeError:\n",
    "            balconydirection = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            apartment_type = soup.find(\"span\", itemprop=\"apartment_type\").text\n",
    "        except AttributeError:\n",
    "            apartment_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "        except AttributeError:\n",
    "            furnishing_sell = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "            \n",
    "        try:\n",
    "            apartment_feature = soup.find(\"span\", itemprop=\"apartment_feature\").text\n",
    "        except AttributeError:\n",
    "            apartment_feature = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"price_m2\": price_m2,\n",
    "            \"rooms\": rooms,\n",
    "            \"toilets\": toilets,\n",
    "            \"direction\": direction,\n",
    "            \"property_status\": property_status,\n",
    "            \"balconydirection\": balconydirection,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"apartment_type\": apartment_type,\n",
    "            \"furnishing_sell\": furnishing_sell,\n",
    "            \"size\": size,\n",
    "            \"apartment_feature\": apartment_feature\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='muaBanCanHo.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('canHoLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    i = 0\n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        i+=1\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL {i}: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data mua bán mặt bằng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[3].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            price_m2 = soup.find(\"span\", itemprop=\"price_m2\").text\n",
    "        except AttributeError:\n",
    "            price_m2 = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            commercial_type = soup.find(\"span\", itemprop=\"commercial_type\").text\n",
    "        except AttributeError:\n",
    "            commercial_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "        except AttributeError:\n",
    "            furnishing_sell = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"price_m2\": price_m2,\n",
    "            \"direction\": direction,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"commercial_type\": commercial_type,\n",
    "            \"furnishing_sell\": furnishing_sell,\n",
    "            \"size\": size,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='matBang1.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('matBangLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    i = 0\n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        i+=1\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL {i}: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data mua bán đất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[3].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            price_m2 = soup.find(\"span\", itemprop=\"price_m2\").text\n",
    "        except AttributeError:\n",
    "            price_m2 = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            land_type = soup.find(\"span\", itemprop=\"land_type\").text\n",
    "        except AttributeError:\n",
    "            land_type = None\n",
    "\n",
    "        try:\n",
    "            pty_characteristics = soup.find(\"span\", itemprop=\"pty_characteristics\").text\n",
    "        except AttributeError:\n",
    "            pty_characteristics = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        try:\n",
    "            width = soup.find(\"span\", itemprop=\"width\").text\n",
    "        except AttributeError:\n",
    "            width = None\n",
    "\n",
    "        try:\n",
    "            length = soup.find(\"span\", itemprop=\"length\").text\n",
    "        except AttributeError:\n",
    "            length = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"price_m2\": price_m2,\n",
    "            \"direction\": direction,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"land_type\": land_type,\n",
    "            \"pty_characteristics\": pty_characteristics,\n",
    "            \"size\": size,\n",
    "            \"width\": width,\n",
    "            \"length\": length,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='muaBanDat1.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('muaBanDatLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    i = 0\n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        i+=1\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL {i}: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data cho thuê căn hộ chung cư"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[0].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            ad_type = soup.find(\"span\", itemprop=\"ad_type\").text\n",
    "        except AttributeError:\n",
    "            ad_type = None\n",
    "            \n",
    "        try:\n",
    "            rooms = soup.find(\"span\", itemprop=\"rooms\").text\n",
    "        except AttributeError:\n",
    "            rooms = None\n",
    "            \n",
    "        try:\n",
    "            toilets = soup.find(\"span\", itemprop=\"toilets\").text\n",
    "        except AttributeError:\n",
    "            toilets = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "            \n",
    "        try:\n",
    "            balconydirection = soup.find(\"span\", itemprop=\"balconydirection\").text\n",
    "        except AttributeError:\n",
    "            balconydirection = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            apartment_type = soup.find(\"span\", itemprop=\"apartment_type\").text\n",
    "        except AttributeError:\n",
    "            apartment_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "        except AttributeError:\n",
    "            furnishing_sell = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        try:\n",
    "            deposit = soup.find(\"span\", itemprop=\"deposit\").text\n",
    "        except AttributeError:\n",
    "            deposit = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"ad_type\": ad_type,\n",
    "            \"rooms\": rooms,\n",
    "            \"toilets\": toilets,\n",
    "            \"direction\": direction,\n",
    "            \"balconydirection\": balconydirection,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"apartment_type\": apartment_type,\n",
    "            \"furnishing_sell\": furnishing_sell,\n",
    "            \"deposit\": deposit,\n",
    "            \"size\": size,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='choThueCanHoChungCu.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('choThueCanHoChungCuLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    \n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data cho thuê nhà ở"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[0].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            ad_type = soup.find(\"span\", itemprop=\"ad_type\").text\n",
    "        except AttributeError:\n",
    "            ad_type = None\n",
    "\n",
    "        try:\n",
    "            rooms = soup.find(\"span\", itemprop=\"rooms\").text\n",
    "        except AttributeError:\n",
    "            rooms = None\n",
    "            \n",
    "        try:\n",
    "            toilets = soup.find(\"span\", itemprop=\"toilets\").text\n",
    "        except AttributeError:\n",
    "            toilets = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "            \n",
    "        try:\n",
    "            floors = soup.find(\"span\", itemprop=\"floors\").text\n",
    "        except AttributeError:\n",
    "            floors = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            house_type = soup.find(\"span\", itemprop=\"house_type\").text\n",
    "        except AttributeError:\n",
    "            house_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "        except AttributeError:\n",
    "            furnishing_sell = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        try:\n",
    "            deposit = soup.find(\"span\", itemprop=\"deposit\").text\n",
    "        except AttributeError:\n",
    "            deposit = None\n",
    "            \n",
    "        try:\n",
    "            living_size = soup.find(\"span\", itemprop=\"living_size\").text\n",
    "        except AttributeError:\n",
    "            living_size = None\n",
    "            \n",
    "        try:\n",
    "            pty_characteristics = soup.find(\"span\", itemprop=\"pty_characteristics\").text\n",
    "        except AttributeError:\n",
    "            pty_characteristics = None\n",
    "            \n",
    "        try:\n",
    "            width = soup.find(\"span\", itemprop=\"width\").text\n",
    "        except AttributeError:\n",
    "            width = None\n",
    "            \n",
    "        try:\n",
    "            length = soup.find(\"span\", itemprop=\"length\").text\n",
    "        except AttributeError:\n",
    "            length = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"ad_type\": ad_type,\n",
    "            \"rooms\": rooms,\n",
    "            \"toilets\": toilets,\n",
    "            \"direction\": direction,\n",
    "            \"floors\": floors,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"house_type\": house_type,\n",
    "            \"furnishing_sell\": furnishing_sell,\n",
    "            \"deposit\": deposit,\n",
    "            \"size\": size,\n",
    "            \"living_size\": living_size,\n",
    "            \"pty_characteristics\": pty_characteristics,\n",
    "            \"width\": width,\n",
    "            \"length\": length,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='choThueNhaO.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('choThueNhaOLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    \n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data cho thuê mặt bằng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[0].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            ad_type = soup.find(\"span\", itemprop=\"ad_type\").text\n",
    "        except AttributeError:\n",
    "            ad_type = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            commercial_type = soup.find(\"span\", itemprop=\"commercial_type\").text\n",
    "        except AttributeError:\n",
    "            commercial_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_sell = soup.find(\"span\", itemprop=\"furnishing_sell\").text\n",
    "        except AttributeError:\n",
    "            furnishing_sell = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        try:\n",
    "            deposit = soup.find(\"span\", itemprop=\"deposit\").text\n",
    "        except AttributeError:\n",
    "            deposit = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"ad_type\": ad_type,\n",
    "            \"direction\": direction,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"commercial_type\": commercial_type,\n",
    "            \"furnishing_sell\": furnishing_sell,\n",
    "            \"deposit\": deposit,\n",
    "            \"size\": size,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='choThueMatBang.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('choThueMatBangLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    \n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data cho thuê đất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[0].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            ad_type = soup.find(\"span\", itemprop=\"ad_type\").text\n",
    "        except AttributeError:\n",
    "            ad_type = None\n",
    "\n",
    "        try:\n",
    "            direction = soup.find(\"span\", itemprop=\"direction\").text\n",
    "        except AttributeError:\n",
    "            direction = None\n",
    "            \n",
    "        try:\n",
    "            property_legal_document = soup.find(\"span\", itemprop=\"property_legal_document\").text\n",
    "        except AttributeError:\n",
    "            property_legal_document = None\n",
    "\n",
    "        try:\n",
    "            land_type = soup.find(\"span\", itemprop=\"land_type\").text\n",
    "        except AttributeError:\n",
    "            land_type = None\n",
    "\n",
    "        try:\n",
    "            width = soup.find(\"span\", itemprop=\"width\").text\n",
    "        except AttributeError:\n",
    "            width = None\n",
    "            \n",
    "        try:\n",
    "            length = soup.find(\"span\", itemprop=\"length\").text\n",
    "        except AttributeError:\n",
    "            length = None\n",
    "\n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        try:\n",
    "            deposit = soup.find(\"span\", itemprop=\"deposit\").text\n",
    "        except AttributeError:\n",
    "            deposit = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"ad_type\": ad_type,\n",
    "            \"direction\": direction,\n",
    "            \"property_legal_document\": property_legal_document,\n",
    "            \"land_type\": land_type,\n",
    "            \"width\": width,\n",
    "            \"length\": length,\n",
    "            \"deposit\": deposit,\n",
    "            \"size\": size,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='choThueDat.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('choThueDatLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    \n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lấy chi tiết data cho thuê phòng trọ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from a single page\n",
    "def extract_data(url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait until price element is available or timeout after 10 seconds\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pyhk1dv')))\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"b\", class_='pyhk1dv').text\n",
    "        except AttributeError:\n",
    "            price = None\n",
    "\n",
    "        try:\n",
    "            address = soup.find_all(\"span\", class_=\"bwq0cbs\")[0].text\n",
    "        except (IndexError, AttributeError):\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            ad_type = soup.find(\"span\", itemprop=\"ad_type\").text\n",
    "        except AttributeError:\n",
    "            ad_type = None\n",
    "\n",
    "        try:\n",
    "            furnishing_rent = soup.find(\"span\", itemprop=\"furnishing_rent\").text\n",
    "        except AttributeError:\n",
    "            furnishing_rent = None\n",
    "            \n",
    "        try:\n",
    "            size = soup.find(\"span\", itemprop=\"size\").text\n",
    "        except AttributeError:\n",
    "            size = None\n",
    "        \n",
    "        try:\n",
    "            deposit = soup.find(\"span\", itemprop=\"deposit\").text\n",
    "        except AttributeError:\n",
    "            deposit = None\n",
    "        \n",
    "        driver.quit()\n",
    "\n",
    "        return {\n",
    "            \"price\": price,\n",
    "            \"address\": address,\n",
    "            \"ad_type\": ad_type,\n",
    "            \"furnishing_rent\": furnishing_rent, \n",
    "            \"deposit\": deposit,\n",
    "            \"size\": size,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data_list, filename='choThuePhongTro.csv'):\n",
    "    if not data_list:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "    keys = data_list[0].keys()  # Use the keys from the first dictionary as headers\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data_list)\n",
    "\n",
    "# Main function to process all URLs and save results\n",
    "def main():\n",
    "    # Replace this with your actual DataFrame or list of URLs\n",
    "    frame = pd.read_csv('choThuePhongTroLinks.csv')  # Assuming you already have the 'Links' data in CSV\n",
    "    all_data = []\n",
    "    \n",
    "    for url in frame['Links']:\n",
    "        data = extract_data(url)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "        print(f\"Processed URL: {url}\")\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
